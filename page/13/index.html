<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="去人迹罕至的地方，留下自己的足迹。">
<meta name="keywords" content="reinforcement learning, deep learning, machine learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Truly">
<meta property="og:url" content="http://yoursite.com/page/13/index.html">
<meta property="og:site_name" content="Truly">
<meta property="og:description" content="去人迹罕至的地方，留下自己的足迹。">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Truly">
<meta name="twitter:description" content="去人迹罕至的地方，留下自己的足迹。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/13/"/>





  <title>Truly</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Truly</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/17/DNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/17/DNN/" itemprop="url">DNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-17T21:36:58-07:00">
                2018-06-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这个是hugo的deep learning的学习<a href="http://www.dmi.usherb.ca/~larocheh/neural_networks/content.html" target="_blank" rel="noopener">tutorial</a>，需要慢慢刷掉，当然cmu 10707 的<a href="https://www.cs.cmu.edu/~rsalakhu/10707/" target="_blank" rel="noopener">slides</a>很多也是参考这个的(Russlan自己说)整理的，你如果找不到CMU 10707的视频你可以看这个，也是极好的。</p>
<p>然后，这里我想整理下最最基本的neural network的公式的推导，自己再推一遍，感觉就不虚：</p>
<h1 id="网络的基本结构："><a href="#网络的基本结构：" class="headerlink" title="网络的基本结构："></a>网络的基本结构：</h1><p><img src="/images/DNNstructure.jpeg" width="50%" height="50%"></p>
<ul>
<li>layer pre-activation for $k &gt; 0$ <strong>$(h_{0}(x) = x)$</strong><br>  $a^{(k)}(x) = b^{(k)} + W^{(k)}h^{(k - 1)}(x)$<br>  感觉这么记忆，不会记乱，k就是第几层neuron，然后$h^{(k - 1)}(x)$是前一层的输出，然后weights $W^{(k)}$，bias $b^{(k)}$都是这层的，虽然有前一层输出作为的输入，但是这些还是算这一层的。</li>
<li><p>hidden layer activation (k from 1 to L)<br>  $h^{(k)}(x) = g(a^{(k)}(x))$<br>  反正这个就是这一层的输出，然后，这层的weights，bias最终都会由这层的输出所终结。</p>
</li>
<li><p>output layer activation $k = L + 1$:<br>  $h^{(L + 1)}(x) = o(a^{(L + 1)}(x)) = f(x)$<br>  和hidden不一样就是这个是最后一层了，然后这层的activation function也会和之前hidden layers有所不同，如果是分类的话，往往是softmax</p>
</li>
<li><p>softmax activation function at the output:<br>  $o(a) = softmax(a) = [\frac{exp(a_1)}{\sum_c exp(a_c)}…\frac{a_C}{\sum_c exp(a_c)}]^T$<br>  如何理解softmax呢，为什么多分类问题里面要用softmax呢，而不是别的呢？<br>  知乎对此有一定的<a href="https://www.zhihu.com/question/40403377" target="_blank" rel="noopener">讨论</a>，我这里引用下王赟(yun第一声)大神的解答：</p>
<ul>
<li>原因之一：希望特征对概率的影响是乘性的</li>
<li>原因之二：多类分类问题的目标函数常常选为cross-entropy，…(推完整个，回来补)</li>
</ul>
</li>
<li><p>activation function:</p>
<ul>
<li><p>sigmoid：</p>
<ul>
<li>formula:<br>  $\sigma(x) = \frac{1}{1 + e^{-x}}$，$\sigma(x)’ = \frac{e^x}{(1 + e^x)^2} = (1 - \sigma(x)) \sigma(x)$</li>
<li>shortcomings:<ul>
<li>gradient vanish</li>
<li>symmetric</li>
<li>time cosuming to compute exp</li>
</ul>
</li>
</ul>
</li>
<li><p>tanh:</p>
<ul>
<li>formula:<br>  $tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1 + e^{-2}} - 1 = 2 \sigma(2x) - 1$，$tanh(x)’ = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</li>
<li>感觉就是解决了原点对称的问题</li>
</ul>
</li>
<li><p>relu:</p>
<ul>
<li>formula：$f(x) = max(0, x)$</li>
<li>优点：<br>  Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生<br>  计算量小</li>
<li>缺点：<br>  部分neuro会死亡</li>
</ul>
</li>
<li><p>leaky relu：</p>
<ul>
<li>formula：$f(x) = max(\epsilon x, x)$</li>
<li>优点:<br>  解决了neuron会死亡的问题</li>
</ul>
</li>
<li><p>maxout：</p>
<ul>
<li>formula: 对 relu 和 leaky relu的一般归纳：$f(x) = max(w_1^T x + b_1, w_2^T x + b_2)$</li>
<li>优点:<br>  计算简单，不会死亡，不会饱和</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function:"></a>loss function:</h1><ul>
<li><p>stochastic gradient descent (SGD):<br>  随机梯度下降应该是最最基础的梯度下降的方法了，</p>
<ul>
<li>initialize  $\theta$ ($\theta = {W^{(1)}, b^{(1)}…，W^{(L + 1)}}$)</li>
<li>algorithm:<br>  for N iterations: (One epoch)<pre><code>for each training example $(x_{(t)}, y_{(t)})$   
    $\delta = -\nabla_{\theta}l(f(x_{(t)}, \theta), \y_{(t)}) - \lambda\nabla_{(\theta)}$
    $\omiga_{(\theta)}$
    $\theta \leftarrow \theta + \alpha \delta$
</code></pre></li>
<li>SGD 的<a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">优缺点</a>：<ul>
<li>缺点：<ul>
<li>选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了</li>
<li>相对BGD noise会比较大</li>
</ul>
</li>
</ul>
</li>
<li><p>batch gradient descent (BGD) 的<a href="https://zhuanlan.zhihu.com/p/25765735" target="_blank" rel="noopener">对比</a>：<br>  所谓batch就是一起算，你看公式就知道：<br>  $\theta \leftarrow \theta + \frac{1}{m}\sum_{i}(y_i - f(x;\theta)(x_i))$ (MSE)</p>
<ul>
<li>缺点：m很大的时候，train的会比较慢</li>
<li>优点：比SGD稳定</li>
</ul>
</li>
<li><p>mini-batch GD:<br>  就是这两个的折中，就像强化学习里面的，TD，Monta Carlo之间的n step-TD</p>
<ul>
<li>advantages:<ul>
<li>give a accurate estimate of average loss</li>
<li>can leverage matrix operations, which cost less than BGD</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>what neural network estimates?<br>  $f(x)_{c} = P(y=c|x)$， where c means which class.</p>
</li>
<li><p>what to optimize?<br>  maximize log likelihood —- minize negative log likelihood: $P(y_i=c|x_i)$，given $(x_i, y_i)$<br>  cross-entropy: p, q (p one-hot, q distribution of the P(y=c|x))<br>  $l(f(x), y) = -\sum_c1(y=c)log f(x)_c = - log f(x)_y$</p>
</li>
</ul>
<h1 id="loss-gradient-output"><a href="#loss-gradient-output" class="headerlink" title="loss gradient output:"></a>loss gradient output:</h1><p><img src="/images/DNN-output-layer-gradient.jpeg" width="50%" height="50%"></p>
<h2 id="loss-gradient-at-output"><a href="#loss-gradient-at-output" class="headerlink" title="loss gradient at output"></a>loss gradient at output</h2><ul>
<li><p>partial derivative:<br>  $\frac{\partial - logf(x)_y}{\partial f(x)_c} = \frac{-1^{(y = c)}}{f(x)^y}$<br>  这里，y要和c一样才有值，因为这里cross-entropy里面用了one-hot，只有在同一维度下面，求偏导才有值。</p>
</li>
<li><p>gradient:<br>  然后，我们推广到，求梯度<br>  $\nabla_{f(x)} -logf(x)_y= \frac{-1}{f(x)_y} [1^{(y=0)}…1^{(y=C-1)}]^T = \frac{-e(y)}{f(x)^y}$</p>
</li>
</ul>
<h2 id="loss-gradient-at-output-pre-activation"><a href="#loss-gradient-at-output-pre-activation" class="headerlink" title="loss gradient at output pre-activation"></a>loss gradient at output pre-activation</h2><ul>
<li><p>partial derivative:<br>  首先还是标量的形式，<br>  $\frac{\partial - logf(x)_y}{\partial a^{(L+1)}(x)_c} = (1^{(y = c)}} - f(x)^y)$<br>  这里，y要和c一样才有值，因为这里cross-entropy里面用了one-hot，只有在同一维度下面，求偏导才有值。</p>
</li>
<li><p>gradient:<br>  然后，我们类比到向量上面，<br>  $\nabla_{a^{(L+1)}(x)_c}[- logf(x)_y}] = -(e^{(y)}} - f(x)^y)$</p>
</li>
<li><p>proof:<br>  这里的proof不完整，只是推了一个维度的，完整的可以参考ece一个师兄的知乎的<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a>，之后回来在补推一下。<br><img src="/images/DNN-output-layer-gradient-proof.jpeg" width="70%" height="70%"></p>
</li>
</ul>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><h2 id="Compute-output-gradient-before-activation"><a href="#Compute-output-gradient-before-activation" class="headerlink" title="Compute output gradient (before activation)"></a>Compute output gradient (before activation)</h2><p>$\nabla_{a^{(L+1)}(x)} -logf(x)_y \leftarrow - (e(y)-f(x))$</p>
<h2 id="for-k-from-L-1-to-1"><a href="#for-k-from-L-1-to-1" class="headerlink" title="for k from L + 1 to 1"></a>for k from L + 1 to 1</h2><ul>
<li>compute gradients of hidden layer parameter<br>$\nabla_{W^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y h^{(k-1)}(x)^T$<br>$\nabla_{b^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below<br>$\nabla_{b^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below<br>$\nabla_{h^{(k-1)}(x)} -logf(x)^y \leftarrow $ $W^{(k)T} \nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below (before activation)<br>$\nabla_{a^{(k-1)}(x)} -logf(x)^y \leftarrow $ $(\nabla_{h^{(k-1)}(x)} -log f(x)^y) \odot […,g’(a^{(k-1)}(x)_j),…]$</li>
</ul>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><ul>
<li><p>L1 &amp; L2 regularization</p>
<ul>
<li>L1 $\frac{\lambda}{2m} \sum |w|^2$<br>  L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).</li>
<li>L2 $\frac{\lambda}{2m} \sum |w|$<br>  Unlike L2, the weights may be reduced to zero here. Hence, it is very useful when we are trying to compress our model. Otherwise, we usually prefer L2 over it. Sparse solution.</li>
</ul>
</li>
<li><p>Dropout<br>  So what does dropout do? At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections as shown below.<br>  <img src="/images/dropout.jpeg" width="70%" height="70%"><br>  So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning.</p>
<p>  Ensemble models usually perform better than a single model as they capture more randomness. Similarly, dropout also performs better than a normal neural network model.</p>
<p>  This probability of choosing how many nodes should be dropped is the hyperparameter of the dropout function. As seen in the image above, dropout can be applied to both the hidden layers as well as the input layers.</p>
</li>
<li><p>Batch Normalization</p>
<ul>
<li>idea<br>  is that since it’s benefit to training if the input data is normalized, so why not normalize in hidden layers to solve the internal covariance shift.</li>
<li>denormalization<br>  to avoid extra effect of normalization, the denormalization parameters are helpful to adjust<br><img src="/images/batch-normalization.jpeg" width="70%" height="70%"></li>
</ul>
</li>
</ul>
<h1 id="Implementation-of-simple-neuron-network"><a href="#Implementation-of-simple-neuron-network" class="headerlink" title="Implementation of simple neuron network"></a>Implementation of simple neuron network</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/16/周末做什么/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/16/周末做什么/" itemprop="url">周末做什么</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-16T19:43:21-07:00">
                2018-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/日常/" itemprop="url" rel="index">
                    <span itemprop="name">日常</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很多人会烦周末了，做什么呢？<br>一些人也许会去出去和朋友浪，一些人也许会花周末的时间一直打游戏，当然也有大神会在周末继续工作。</p>
<p>我感觉把，周末最需要的是休息吧，作息规律，妥善的饮食，都是重要的。我一直觉得所谓休息不是放纵自己的欲望，一个人幸福也许是来自他/她对自己欲望的驾驭，熬夜刷剧不叫休息，熬夜看世界杯也不叫休息吧，感觉就是放纵，算是对身体的一种伤害，休息应该是不是伤害身体的。娱乐当然需要呀，晚上好好睡觉，白天起来刷剧不是更爽吗。所以，感觉休息是身体的调整吧。当然，还有精神的放松，以及卸下压力，卸下你这一周的重担。</p>
<p>当然，我感觉你觉得休息够了，你自然可以考虑工作的事情，个人发展的事情，也都挺好的，感觉这个时候更重要的是反思吧，反思这一周，所谓：学而不思则罔吧。你也可以推进一下你需要的长期的要做的事情，你可以整理一下你这一周的得失，以及对下一周的展望。</p>
<p>总之，周末就是一个节点，一个驿站，周而复始，你就会达到你所向往的未来。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/14/进阶之路/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/14/进阶之路/" itemprop="url">进阶之路</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-14T13:59:17-07:00">
                2018-06-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/思考/" itemprop="url" rel="index">
                    <span itemprop="name">思考</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇已经看了好几遍了，但是每读每新：<br>作者：田渊栋<br>链接：<a href="https://www.zhihu.com/question/30022694/answer/224543003" target="_blank" rel="noopener">https://www.zhihu.com/question/30022694/answer/224543003</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>追求数目没有意义。读文章一般两个目的：</p>
<ol>
<li><p>看大家在做什么，找方向。这时候一般读读Abstract和Introduction，对领域有初步了解，知道它主要关于什么，搞清一些概念的含义和联系。这时候不懂没关系，多看几篇文章就懂了。一般聪明的人这一步可以做得很快。</p>
</li>
<li><p>搞清细节找一个自己感兴趣的方向精读，把里面的课题思路和推理细节搞明白，并且还要顺藤摸瓜找到其它大量的相关文献继续读下去。标准是在脑里能有对这个领域有清楚的脉络，能做到独立完成大部分推导和证明。一个靠谱的检查方法是给同组的人或者导师做个讲座，看他们能听懂不。很多时候自己以为懂了，其实和别人一说马上就露出马脚。同时讨论也可以激发新思路，说不定就能找到下一篇文章的出发点。这一步往往会花费一个科研人员大量时间，也是业余和职业科研的关键区别所在。总之分配给每篇文章的时间天差地别。烂文几秒钟就可以放弃，而经典文章还需要每过一阵子回头再去看一看想一想。至于如何评判文章质量，那就得要靠长年科研积累出来的品味了。接下来的两个阶段就不是光看论文可以看出来的。</p>
</li>
<li><p>写代码实现别人的工作，并且改进每篇文章都会有意或者无意抬高自己贬低别人，都存在一些有意或者无意隐藏的细节，这些不亲手做是看不到的。所以得要动手花时间去实现别人的方法，想方设法达到别人的效果，然后反过来再看看文章。时间长了马上就会学到故意隐藏的蛛丝马迹，理解别人留白的道理。光看文章的话，这类经验的积累要慢很多。一般说的“纸上谈兵”就是指这一步没做。我在15年1月刚去Facebook AI Research的时候，在深度学习上还没有实际操作经验。交给我的第一件事情是复现VGG在ImageNet上的性能，那时还没有BatchNorm，跑5个有2个能开始收敛的就不错了，最后花了几周搞定了。整个过程让我学到不少经验。</p>
</li>
<li><p>总结经验，融会贯通，找到并且遵循自己的方法论重复3很多次之后，可能会觉得自己比较有经验了。别人问起的时候也能侃侃而谈，但说的往往是一些分散且孤立的经验。并且你会发现自己很容易遗忘这些经验，这个并不是因为记忆力不好，而是因为思路不系统。这个就需要反复思考反复提炼，从而形成自己的方法论。有了方法论之后，心里就有大方向而不会随便乱试乱撞，效率就会高很多，并且能在一个科研方向上挖很深坚持很久，而不是哪个课题热做哪个。在指导别人的时候也可以做到有的放矢。在这个基础上再看文献，往往就会读懂很多一开始读不懂的东西。比如说为什么作者要强调A而否认B，那是因为他相信A后面的哲学和方法论。</p>
</li>
</ol>
<p>如果你发现自己提炼不了，或者本来知识就是凌乱的，那么要么就是(1)境界未到，要么就是(2)领域还没有成熟，目前的知识点只是零碎的拼凑。(1)要靠自己练，(2)则预示着大机遇，一个研究者牛不牛就看他是不是可以在别人都放弃的地方找到新的规律。</p>
<p>一般完成1是新闻及科普的水平，2到3是博士生低年级至高年级的水平，精通3到初入4是博后的水平，精通4则是研究员和教授的水准。另外，从1到4并没有特别固定的顺序，可能你在某个领域是4，另一个领域还只是1或2的程度；或者你在4中获得的经验能反过来帮助1和2（这个很常见）；或者一上来就可以跳过2做3，然后等3有了结果之后再去补2，等等。当然，一步跳到4那是民科的水平。</p>
<p>然后，看看我自己，应该还初入2, 3这个阶段吧，需要继续努力呀！！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/批量重命名/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/批量重命名/" itemprop="url">批量重命名图片文件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T20:51:35-07:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tech/" itemprop="url" rel="index">
                    <span itemprop="name">tech</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自灰羽吖大大CSDN的<a href="https://blog.csdn.net/wearge/article/details/77374150" target="_blank" rel="noopener">博客</a>：<br>其实只要对os这个包熟悉便不难，对于人脸识别项目，有些图片可能来自其他途径，这些图片常用作测试，但是对于外来图片存在命名问题。这篇就讲一下怎么实现批量重命名图片等其他文件</p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">path_name=&apos;/home/huiyu/PycharmProjects/faceCodeByMe/testdata&apos;</span><br><span class="line">#path_name :表示你需要批量改的文件夹</span><br><span class="line">i=0</span><br><span class="line">for item in os.listdir(path_name):#进入到文件夹内，对每个文件进行循环遍历</span><br><span class="line">    os.rename(os.path.join(path_name,item),os.path.join(path_name,(str(i)+&apos;.jpg&apos;)))#os.path.join(path_name,item)表示找到每个文件的绝对路径并进行拼接操作</span><br><span class="line">    i+=1</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/LearnAdaptSeg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/LearnAdaptSeg/" itemprop="url">Learning to Adapt Structured Output Space for Semantic Segmentation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T17:00:49-07:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://arxiv.org/abs/1802.10349" target="_blank" rel="noopener">这篇论文</a>对应的<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">代码</a>是最近在跑的，需要好好看看。</p>
<h1 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h1><p>Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, <strong>we adopt adversarial learning in the output space</strong>. To further enhance the adapted model, we construct a <strong>multi-level adversarial network</strong> to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.</p>
<h1 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h1><h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><ul>
<li>1) a segmentation model to predict output results</li>
<li>2) a discriminator to distinguish whether the input is from the source or target segmentation output. <h2 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h2></li>
<li>propose a domain adaptation method for pixel-level semantic segmentation via adversarial learning</li>
<li>demonstrate that adaptation in the output (segmentation) space can effectively align scene layout and local context between source and target images</li>
<li>a multi-level adversarial learning scheme is developed to adapt features at different levels of the segmentation model, which leads to improved performance.</li>
</ul>
<h1 id="structure"><a href="#structure" class="headerlink" title="structure"></a>structure</h1><p>感觉也不是那么难理解，就是有点像u-net，它这里的话就是在不同的layer里面用GAN，就是所谓的multi-，<br>然后主要是output space上面，因为这篇文章发现，不管两个domain的图多么不一样，他们在output space总是有很多相似的地方。<br><img src="/images/multiGAN.jpeg" alt=""></p>
<h1 id="model-overview"><a href="#model-overview" class="headerlink" title="model overview"></a>model overview</h1><p>这个结构有两个模块：生成器$G$和判别器$D_i$ （i 表示是第几层的判别器）。images通过生成器出来的是源域segmentation的概率分布$P_s$</p>
<h1 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h1><p>$L(I_s, I_t) = L_{seg}(I_s) + \lambda L_{adv}(I_t)$</p>
<ul>
<li>$L_{seg}(I_s)$<br>  cross-entropy loss using ground truth annotations in the source domain</li>
<li>$L_{adv}$<br>  对抗损失，用来使得源域的预期的数据分布和目标域相近</li>
<li>$\lambda_{abv}$<br>  这个weight用来平衡这两个loss</li>
</ul>
<h1 id="Output-space-adaptation"><a href="#Output-space-adaptation" class="headerlink" title="Output space adaptation"></a>Output space adaptation</h1><h2 id="Single-level-adversarial-learning"><a href="#Single-level-adversarial-learning" class="headerlink" title="Single-level adversarial learning"></a>Single-level adversarial learning</h2><h3 id="Discriminator-Training"><a href="#Discriminator-Training" class="headerlink" title="Discriminator Training"></a>Discriminator Training</h3><ul>
<li>cross entropy<ul>
<li>定义：<br>  给定两个分布，p，q，它们在给定样本集上面的交叉熵的定义如下<br>  $CEH(p, q) = E_p[-log q] = - \sum_{x \in X}p(x)q(x) = H(p) + D_{KL}(p||q)$，当p的熵给定时，交叉熵和KL散度是一致是的，一定程度上可以用来描述，这两个分布的距离。</li>
<li>讨论：讲到cross entropy，为什么用cross entropy loss 于分类呢？(<a href="http://jackon.me/posts/why-use-cross-entropy-error-for-loss-function/" target="_blank" rel="noopener">Jackon解答</a>)<ul>
<li>比起一般的classification error 作为loss，它很更精细准确的去描述model的优劣</li>
<li>比起MSE，来说，它是一个凸优化的问题</li>
</ul>
</li>
</ul>
</li>
<li><p>segmentation softmax output:<br>  $P = G(I) \in R^{HxWxC}$, 这里C是种类数，这里C是2，来自源域或者来自目标域</p>
</li>
<li><p>cross-entropy loss：<br>  我们将P传到全卷积的判别器D里面：$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$</p>
</li>
</ul>
<h3 id="Segmentation-Network-Training"><a href="#Segmentation-Network-Training" class="headerlink" title="Segmentation Network Training"></a>Segmentation Network Training</h3><ul>
<li>segmentation loss:<br>  在源域的话我们正常训练，还是由cross-entropy loss来定义：$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</li>
<li>adversarial loss：<br>  在目标域，我们的对抗损失是：$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$，这个损失是用来欺骗判别器的，使得两者的预期的概率的一致</li>
</ul>
<h2 id="Multi-level-Adversarial-Learning"><a href="#Multi-level-Adversarial-Learning" class="headerlink" title="Multi-level Adversarial Learning"></a>Multi-level Adversarial Learning</h2><ul>
<li>multi-level loss<br>  就是在low-level的feature space里面加上上面的loss，也不是很难理解：<br>  $L_{I_s, I_t} = \sum_i \lambda_i^{seg}(I_s) + \sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，i表示第几层网络。</li>
<li>whole picture<br>  有了上面的对loss的介绍后，我们的问题其实就是一个min-max的优化问题：<br>  $max_D min_G L(I_s, I_t)$</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/自省/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/自省/" itemprop="url">自省</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T10:08:55-07:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/思考/" itemprop="url" rel="index">
                    <span itemprop="name">思考</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>重温了一下知乎上田渊栋大大的感悟：常常看来提醒自己：</p>
<p>作者：田渊栋<br>链接：<a href="https://zhuanlan.zhihu.com/p/26178137" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26178137</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>思危，思退，思变</p>
<p>首先要自律。这是最基本的。如果一个人不能控制自己的行为，那无法走出自己的路。像说到要做到，有规律地锻炼身体，勤奋努力，这些都是重要的。</p>
<p>接下来要跳出舒适区。举个例子，勤奋本来就是舒适区的一种。“勤能补拙”这个词是很好的，首先它说明现在处于“拙”的状态中，需要继续努力；其次，勤只能用来“补”拙，而不能让拙变巧，不是根本的解决方案。勤奋是一种惶惶然的状态，而不是一种满足的状态，勤奋意味着自己不如别人，于是得要花更多的时间去补救，别人干八小时就够了，自己得花十几个小时才能赶上。这种状态是不长久的，碰到出些小毛小病，或者家里有事，那就补不过来，就要掉队了。勤奋的用处是试错，是让自己在落后时可以多花点时间找到正确的方法，以达到和别人相当甚至更高的效率，从而提高自己的能力。别人是科班出身，我半路杀进来，当然要多花时间去补；别人学习效率高，我的效率不高，于是得要多花点时间去探索更好的方法。勤奋是暂态，它最终目的是找到更好的方法及时补上以离开这个状态，而不是以一直维持这个状态为荣。</p>
<p>不能按部就班，要随时作好把棋盘翻过来的准备，世事变化很快，以前的所有努力，不管经历过什么样的辛劳，全都是沉没成本，在必要时候都需要扔掉的，或许做了很多年方向A，时势告诉你情况不妙，要换成方向B，那就得坚决换。以前或许这个不常见，但是以后这样的事情会越来越多。干了十年方向A，人工智能把方向A吃掉了，然后马上转做方向B，做了五年，人工智能再把B吃掉，然后继续，如此往复。很多时候转变不是一朝一夕，而是靠滴水穿石的功夫，今天长进一点，明天长进一点，跟着领域一起变，若是一个人跑得比别人快，他就会在市场上稀缺并因此获得相对的安全。做研究的人都习惯这个，每天看新东西，每天打开思路，时刻承认自己老旧了几个月或者几周，马上拍拍屁股跟上。学会了这些再去教徒弟是饿不死师傅的，反而让师傅变得更厉害，因为师傅主动跳出来接受打脸，学得比徒弟快。对很多人来说这个比较难，特别是一直顺风顺水的。但若是一直不敢看外面的世界，那迟早有一天会被逼进去面对。与其被逼，不如提早一些主动跳进去。历史无数次以血的教训告诉了大家，适者生存乃永恒之铁律，人类在千万物种中杀出血路成为地球的主宰，也必将背负着这样的命运走向未来。一个人逃避，这个人会被淘汰，一个领域逃避，这个领域会被淘汰，一个国家逃避，这个国家就会被淘汰。</p>
<p>任何时候，自己一定是有错的，最可怕的不是自己错了，而是不知道自己哪里错，并且在错的方向上越走越远。如果周围有厉害的同事，这种感觉尤其强烈，碰上了随便讨论两句，就知道自己哪些知识不足，暗地里记下马上回去补。为此，主动发言积极讨论是很重要的，思维有碰撞才知道问题在哪里。我有时候觉得自己一直在悬崖边上走，也许之前走得还行，但那都过去了，下一步随时有可能踏进崖边的泥地即将摔倒。踏错了不要紧，及时发现自己错了收脚就行；怕的是一直走安稳的道，连悬崖长什么样都不知道了。前辈和老师们说的话，也非常有可能是错的，而身为后辈的我们，大任在于如何找出他们的错误来。找出了的话，能力就得到了提升。而自信，往往就是通过这种方式磨练出来的——为什么自己和别人不同？因为选了一条不一样的路。</p>
<p>再往上走，主次是要分清的。重要的要抓牢，不重要的要放手。有人读过很多文章看过很多书，勤勉自律好学爱问样样不缺，但门门都不精；有人事事亲为，务求完美，大事上往往把握不了。短木板理论是有问题的，大部分岗位不需要全才，要的是一专多能，要的是某方向很牛非常牛，相同程度的可以掰指头数过来，其它的过线就行，甚至不达标也无人关心。如果你不是专家，高薪聘请没有意义；如果你是专家，求全责备没有意义。人一天只有24小时，所以知道哪里要放弃是很重要的，很多时候，没有牺牲就没有得到，要得到就得付出代价，事前权衡利弊，事后愿赌服输。输了不要紧，再来一次。当然在现实中并没有那么惨烈的权衡，而往往是找到了自己的方向，自然而然地就向这个方向发展下去，这时候主要的阻力，就在于登顶之难而非选择之痛。然而，即便生于风平浪静的和平年代，觉悟依然要有，或许将来有一天，得要做出这样的决断。</p>
<p>最后，不要在优越感中停止自己的脚步。名利于人最可怕的莫过于此，分明刚刚启程，但欢呼声让你觉得已然冲线，本来要万里长征，却变成了百米短跑，接受完了鲜花之后，就再也看不见远处的风景。其实境界到了或是未到，只有自己知道。跋山涉水，风餐露宿，鼓掌的是别人，度化的是自己。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/12/舒适区/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/12/舒适区/" itemprop="url">work and life balance</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T21:21:50-07:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/日常/" itemprop="url" rel="index">
                    <span itemprop="name">日常</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>计划开始推进的第二天，感觉自己有点点紧张，也感受到了压力，找工作和research，就像以前的due一样，加到自己身上。坚持就好了，承担更多的事情，自然要承受更大的压力，对应需要适当的娱乐和运动来平衡，自己一直在坚持游泳，坚持就好了，也许可以去听听音乐会。心里的信念和自己所向往的东西，一直支撑着自己往前走。记得田渊栋大大说：写下来，往前走。对，就是这样子。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/12/CycleGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/12/CycleGAN/" itemprop="url">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T14:10:48-07:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>前面几篇主要都是采用了风格迁移的思想，<a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">这篇论文</a>主要就是讲这个：</p>
<h1 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h1><p>Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we cou- ple it with an <strong>inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F(G(X)) ≈ X (and vice versa)</strong>. Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.</p>
<h1 id="unpaired"><a href="#unpaired" class="headerlink" title="unpaired"></a>unpaired</h1><p>他们要解决的unpaired translation，换句话说就是没有，很多label的，如下图所示：<br><img src="/images/unpaired.jpeg" alt=""></p>
<h1 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h1><p>感觉这篇论文最核心的思想就是这个cycle consistency loss，大意是，做translation，从一个句子，从英语翻译到法语，在从法语翻译过来，应该和原句一样才对，反之亦然。cycle consistency loss主要是采用这个思想：<br>$G: X \rightarrow Y, F: Y \rightarrow X, F(G(x)) \approx x, G(F(y))) \approx y$</p>
<p>首先我们在重温一下GANs，它最核心的一点是一个adversarial loss来使得生成的图片和真实的图片难以被区分。<br>结构如下：<br><img src="/images/GANexplanation.jpeg" alt=""></p>
<p>然后我们在来看看这篇论文的loss，就会很清晰了，不就是是两个GAN吗，然后结合了这个cycle consistency loss<br><img src="/images/cycleconsisloss.jpeg" alt=""></p>
<h2 id="adversarial-loss"><a href="#adversarial-loss" class="headerlink" title="adversarial loss"></a>adversarial loss</h2><p>$L_{GAN}(G, D_{Y}, X, Y) = E_{y \sim p_{data}(y)}[log D_{Y}(y)] + E_{x \sim p_{data}(x)}[log(1 - D_{Y}(G(x)))]$<br>$L_{GAN}(F, D_{X}, X, Y) = E_{x \sim p_{data}(x)}[log D_{X}(x)] + E_{y \sim p_{data}(y)}[log(1 - D_{X}(F(y)))]$</p>
<h2 id="cycle-consistency-loss"><a href="#cycle-consistency-loss" class="headerlink" title="cycle consistency loss"></a>cycle consistency loss</h2><p>$L_{cyc}(G, F) = E_{x \sim p_{data}(x)}[|F(G(x)) - x |] + E_{y \sim p_{data}(y)}[|G(F(y)) - y |]$</p>
<p>其实总的idea不是很难理解，看懂这个在看别的论文就会好很多，很多论文都是基于cycle-GAN来改的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/12/conda路径/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/12/conda路径/" itemprop="url">conda 和系统 python 路径问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T09:58:22-07:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tech/" itemprop="url" rel="index">
                    <span itemprop="name">tech</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>conda装完，在~/.bashrc 里面加上它的路径，source完就会取代系统自己的python路径有时候会不方便，我找到了<a href="https://blog.csdn.net/zhangxinyu11021130/article/details/64125058" target="_blank" rel="noopener">这篇博客</a>，以后可以参考一下，以下为转发内容：</p>
<p>那么，如果在~/.bashrc中修改文件的话，即加入export PATH=”/home/myname/anaconda2/bin:$PATH”，则输入python命令就会直接出来Anaconda环境下的python，当然，用TensorFlow是好的，用caffe就。。。。。。。因此，用caffe的话，我就会把这行去掉，再重新source ~/.bashrc.</p>
<p>这样做是麻烦了一些，但是那怎么办呢？我就想在caffe下使用自带python，在tensorflow的时候使用Anaconda。</p>
<p>于是，学到了一个“技巧”，叫做别名声明alias。</p>
<p>具体用法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alias py27=&quot;/usr/bin/python2.7&quot; </span><br><span class="line">alias pyana=&quot;/home/myname/anaconda2/bin/python2.7&quot; (我自己的，一定要精确到python的版本，不能只到文件夹)</span><br></pre></td></tr></table></figure></p>
<p>这样在使用系统自带python的时候，只需要在命令行输入py27即可，用Anaconda，输入python或者pyana都可。</p>
<p>注意： </p>
<ol>
<li>~/.bashrc的文件中，export PATH=”/home/myname/anaconda2/bin:$PATH”还是要加上的 </li>
<li>上面两行在每次开机的时候都是要输入的，如果不想每次都输入，则要将这两行加入~/.bashrc的文件中 </li>
<li>如果想取消别名声明，用unalias py27或者删除~/.bashrc中的PATH，并且重新source ~/.bashrc</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/11/leetcode338/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/11/leetcode338/" itemprop="url">388. Longest Absolute File Path</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-11T18:29:30-07:00">
                2018-06-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>从今天开始刷google leetcode tag，应该都是先从抄开始的吧，先求速度吧。还是那句话，难的不是如何开始，而是坚持吧。作息，情绪，运动，娱乐，压力的平衡和统一，还是不是很容易。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chu Lin</p>
              <p class="site-description motion-element" itemprop="description">去人迹罕至的地方，留下自己的足迹。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">164</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">94</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chu Lin</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
