<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="去人迹罕至的地方，留下自己的足迹。">
<meta name="keywords" content="reinforcement learning, deep learning, machine learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Truly">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Truly">
<meta property="og:description" content="去人迹罕至的地方，留下自己的足迹。">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Truly">
<meta name="twitter:description" content="去人迹罕至的地方，留下自己的足迹。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/"/>





  <title>Truly</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Truly</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/leetcode766/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/26/leetcode766/" itemprop="url">leetcode766</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-26T01:52:56-04:00">
                2018-06-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>A matrix is Toeplitz if every diagonal from top-left to bottom-right has the same element. Now given an M x N matrix, return True if and only if the matrix is Toeplitz.</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">matrix = [</span><br><span class="line">  [1,2,3,4],</span><br><span class="line">  [5,1,2,3],</span><br><span class="line">  [9,5,1,2]</span><br><span class="line">]</span><br><span class="line">Output: True</span><br><span class="line">Explanation:</span><br><span class="line">In the above grid, the diagonals are:</span><br><span class="line">&quot;[9]&quot;, &quot;[5, 5]&quot;, &quot;[1, 1, 1]&quot;, &quot;[2, 2, 2]&quot;, &quot;[3, 3]&quot;, &quot;[4]&quot;.</span><br><span class="line">In each diagonal all elements are the same, so the answer is True.</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">matrix = [</span><br><span class="line">  [1,2],</span><br><span class="line">  [2,2]</span><br><span class="line">]</span><br><span class="line">Output: False</span><br><span class="line">Explanation:</span><br><span class="line">The diagonal &quot;[1, 2]&quot; has different elements.</span><br></pre></td></tr></table></figure>
<h1 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h1><p>直接遍历，不过是要check遍历元素和斜对的元素是不是一样</p>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def isToeplitzMatrix(self, matrix):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type matrix: List[List[int]]</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if not matrix or len(matrix) == 0:</span><br><span class="line">            return False</span><br><span class="line">        </span><br><span class="line">        rows = len(matrix)</span><br><span class="line">        cols = len(matrix[0])</span><br><span class="line">        </span><br><span class="line">        for r in range(rows):</span><br><span class="line">            for c in range(cols):</span><br><span class="line">                if (r + 1 &lt;= rows -1 and c + 1 &lt;= cols - 1) and matrix[r][c] != matrix[r + 1][c + 1]:</span><br><span class="line">                    return False</span><br><span class="line">        </span><br><span class="line">        return True</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/lintcode900/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/26/lintcode900/" itemprop="url">lintcode900</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-26T01:37:02-04:00">
                2018-06-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Given a non-empty binary search tree and a target value, find the value in the BST that is closest to the target.</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p>Given root = {1}, target = 4.428571, return 1.</p>
<h1 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h1><p>思路很简单，就是找到通过检索BST找到upper bound和lower bound，最后在比较谁是离得最近的。upper bound的意思是在整棵树里面，比target小却最大的数，lower bound的意思是在整棵树里面比target大的最小的数。这个大小关系是，进一步递归检索的关键。</p>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">Definition of TreeNode:</span><br><span class="line">class TreeNode:</span><br><span class="line">    def __init__(self, val):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left, self.right = None, None</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param root: the given BST</span><br><span class="line">    @param target: the given target</span><br><span class="line">    @return: the value in the BST that is closest to the target</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def closestValue(self, root, target):</span><br><span class="line">        # write your code here</span><br><span class="line">        if root is None:</span><br><span class="line">            return</span><br><span class="line">        </span><br><span class="line">        lower_node = self.lower_bound(root, target)</span><br><span class="line">        upper_node = self.upper_bound(root, target)</span><br><span class="line">        </span><br><span class="line">        if lower_node is None:</span><br><span class="line">            return upper_node.val</span><br><span class="line"></span><br><span class="line">        if upper_node is None:</span><br><span class="line">            return lower_node.val    </span><br><span class="line">    </span><br><span class="line">        if lower_node and upper_node:</span><br><span class="line">            if target - lower_node.val &gt; upper_node.val - target:</span><br><span class="line">                return upper_node.val</span><br><span class="line">            else:</span><br><span class="line">                return lower_node.val</span><br><span class="line">            </span><br><span class="line">        return None</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    # find the node with the largest value that smaller than target</span><br><span class="line">    def lower_bound(self, root, target):</span><br><span class="line">        if root is None:</span><br><span class="line">            return None</span><br><span class="line">        </span><br><span class="line">        # the expect val should be smaller than or equal to target</span><br><span class="line">        if root.val &gt; target:</span><br><span class="line">            return self.lower_bound(root.left, target)</span><br><span class="line">        </span><br><span class="line">        # the largest val</span><br><span class="line">        lower_node = self.lower_bound(root.right, target)</span><br><span class="line">        if lower_node:</span><br><span class="line">            return lower_node</span><br><span class="line">        </span><br><span class="line">        # if lower_node is None, the there&apos;s no node larger than root, so return root</span><br><span class="line">        return root</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # find the node with the smallest value that larger than or equal to target</span><br><span class="line">    def upper_bound(self, root, target):</span><br><span class="line">        if root is None:</span><br><span class="line">            return None</span><br><span class="line">        </span><br><span class="line">        # the expect val should be larger than or equal to target</span><br><span class="line">        if root.val &lt; target:</span><br><span class="line">            return self.upper_bound(root.right, target)</span><br><span class="line">        </span><br><span class="line">        # the smallest val</span><br><span class="line">        upper_node = self.upper_bound(root.left, target)</span><br><span class="line">        if upper_node:</span><br><span class="line">            return upper_node</span><br><span class="line">            </span><br><span class="line">        return root</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/lintcode152/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/26/lintcode152/" itemprop="url">lintcode152</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-26T00:01:05-04:00">
                2018-06-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Given two integers n and k, return all possible combinations of k numbers out of 1 … n.</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p>Given <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">[</span><br><span class="line">  [2,4],</span><br><span class="line">  [3,4],</span><br><span class="line">  [2,3],</span><br><span class="line">  [1,2],</span><br><span class="line">  [1,3],</span><br><span class="line">  [1,4]</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param n: Given the range of numbers</span><br><span class="line">    @param k: Given the numbers of combinations</span><br><span class="line">    @return: All the combinations of k numbers out of 1..n</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def combine(self, n, k):</span><br><span class="line">        # write your code here</span><br><span class="line">        self.res = []</span><br><span class="line">        tmp = []</span><br><span class="line">        self.dfs(n, k, 1, 0, tmp)</span><br><span class="line">        return self.res</span><br><span class="line">        </span><br><span class="line">    # 递归的定义：在 [1,2...n] 中找到所有以 tmp 开头的的集合，并放到 res里面</span><br><span class="line">    # cur_start_pos: 表示当前要开始的位置</span><br><span class="line">    # visited_num: 表示已经遍历过的数目</span><br><span class="line">    def dfs(self, n, k, cur_start_pos, visited_num, tmp):</span><br><span class="line">        # 递归的出口:</span><br><span class="line">        if visited_num == k:</span><br><span class="line">            self.res.append(tmp[:])</span><br><span class="line">            return</span><br><span class="line">        # 递归的拆解：</span><br><span class="line">        for i in range(cur_start_pos, n + 1):</span><br><span class="line">            # 将当前的element放在tmp set里面: [1] -&gt; [1, 2]</span><br><span class="line">            tmp.append(i)</span><br><span class="line">            # 进入下一层: n = 4, k = 2, next_start_pos = i + 1 = 3 注意是在tmp最后的一个元素后面开始, visited_num_now = 2, </span><br><span class="line">            self.dfs(n, k, i + 1, visited_num + 1, tmp)</span><br><span class="line">            # 回溯: [1, 2] -&gt; [1]</span><br><span class="line">            tmp.pop()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/25/那些高山们/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/25/那些高山们/" itemprop="url">那些高山们</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T21:40:55-04:00">
                2018-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/思考/" itemprop="url" rel="index">
                    <span itemprop="name">思考</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>来到CMU，看到了很多高山，常常心生攀爬的欲望，但是每每心生怯懦。就像动漫<a href="https://search.bilibili.com/bangumi?keyword=overload" target="_blank" rel="noopener">overload</a>里面的人类强者布莱恩面对吸血鬼始祖夏尔提亚那样，有时候会心生绝望，自己毕生淬炼的一击也只能碰到人家的指甲盖。也许，只是自己太希望自己变得更强了，太希望能和他们一起交流一起在一个平台上共事。我不知道有生之年能不能做到，之前也有过一些机会，但是自己也没有好好把握，同时也走了一些弯路。但是不管怎么样，我还是找寻到了自己的目标，自己的一个方向，来到了这里。人生还有好几十年，自然不能放弃。也许，在overload第二季一样，对于布莱恩来说，他用自己的方式，伤到了夏尔提亚的指甲盖，突破了自己。但，也许更适合自己的想法就是像克莱姆一样，不管自己天赋如何，向强者学习，走好自己的步子，去保护他的公主。总之，我会一直努力，也许能看到你们的背影，也许也能触及到你们的背影。总之，能在这里遇到你们，自己真的是太幸福了。期待未来的自己能和你们再次相遇。</p>
<p><img src="/images/克莱恩.jpeg" width="50%" height="50%"><br><img src="/images/布莱恩.jpeg" width="50%" height="50%"><br><img src="/images/RSS.jpeg" width="50%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/24/multi-level-GAN-code解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/24/multi-level-GAN-code解读/" itemprop="url">multi-level-GAN-code解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-24T11:24:59-04:00">
                2018-06-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>接下来我开始解读，<a href="https://arxiv.org/abs/1802.10349" target="_blank" rel="noopener">这篇</a>论文的<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">code</a></p>
<h1 id="Network-Structure-and-Training"><a href="#Network-Structure-and-Training" class="headerlink" title="Network Structure and Training"></a>Network Structure and Training</h1><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>For the discriminator, we use an architecture similar to but utilize all fully-convolutional lay- ers to <strong>retain the spatial information</strong>.<br>总是来说就是，5层卷积网络，kernel是4 x 4 stride是2，channel是{64, 128, 256, 512, 1}。<br>除了最后一层卷积层，其他所有层都是用参数为0.2的leaky ReLU。在最后一层卷积层之后加了一个up-sampling的层，使得最后一层和输入图片的大小是一样的。他们没有使用batch-normalization层，因为他们用小的batch size一起训练判别器和分割网络。(?)</p>
<ul>
<li>batch normalization:</li>
</ul>
<h2 id="Segmentation-Network"><a href="#Segmentation-Network" class="headerlink" title="Segmentation Network"></a>Segmentation Network</h2><p>他们用DeepLab-v2 和 ResNet-101来作为他们分割的baseline，由于memory的问题，他们没有使用multi-scale。<br>他们去掉了最后的分类的一层，然后将最后的两层卷积stride从2改成1。这使得输出的feature maps是输入图片大小的1/8。为了使这个更大，他们在conv4和conv5用了stride分别是2，4的dilated conv。这后面又用了Atrous Spatial Pyramid Pooling (ASPP)作为最后的分类器。在ASPP后面，他们也采用了输出的是softmax的up-sampling层，这层输出的大小和输入的图片大小也是一样的。</p>
<ul>
<li>ASPP:</li>
</ul>
<h2 id="Multi-level-Adaptation-Model"><a href="#Multi-level-Adaptation-Model" class="headerlink" title="Multi-level Adaptation Model"></a>Multi-level Adaptation Model</h2><p>上面的构成了他们single-level的网络结构。为了构建multi-level的结构，他们将conv4的feature map和一个作为辅助分类器的ASPP模块相结合。和single-level类似，这里面也加了一个同样结构的判别器来进行对抗学习。如图：<br><img src="/images/multi-GAN-structure.jpeg" width="70%" height="70%"></p>
<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>作者发现，将segmentation network和discriminitor一起训练效率会比较高。<br>对源域将图片$I_s$向前传最后得到$P_s$，以及优化$L_{seg}$。对于目标域，我们将得到的$P_t$和$P_s$一起输入到判别器里面，然后优化$L_{d}$。此外，对于$P_t$，我们还需要计算对抗损失$L_{ad}$。</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul>
<li><p>whole objective:<br>  $L(I_s, I_t) = L_{seg}(I_s) + \lambda L_{adv}(I_t)$</p>
<ul>
<li>$L_{seg}(I_s)$<br>  cross-entropy loss using ground truth annotations in the source domain</li>
<li>$L_{adv}$<br>  对抗损失，用来使得源域的预期的数据分布和目标域相近</li>
<li>$\lambda_{abv}$<br>  这个weight用来平衡这两个loss</li>
</ul>
</li>
<li><p>discriminitor:</p>
<ul>
<li><p>segmentation softmax output:<br>  $P = G(I) \in R^{HxWxC}$, 这里C是种类数，这里C是19</p>
</li>
<li><p>cross-entropy loss：<br>  我们将P传到全卷积的判别器D里面：$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，这个是binary cross entropy，这里z = 0，表示来自target，z = 1表示来自source</p>
</li>
</ul>
</li>
<li><p>segmentation network:</p>
<ul>
<li>segmentation loss:<br>  在源域的话我们正常训练，还是由cross-entropy loss来定义：$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</li>
<li>adversarial loss：<br>  在目标域，我们的对抗损失是：$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$，这个损失是用来欺骗判别器的，使得两者的预期的概率的一致</li>
</ul>
</li>
<li><p>multi-level:</p>
<ul>
<li>multi-level loss<br>  就是在low-level的feature space里面加上上面的loss，也不是很难理解：<br>  $L_{I_s, I_t} = \sum_i \lambda_i^{seg}L^i_{seg}(I_s) + \sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，i表示第几层网络。</li>
</ul>
</li>
</ul>
<h1 id="Network-Code"><a href="#Network-Code" class="headerlink" title="Network Code"></a>Network Code</h1><h2 id="Discriminitor"><a href="#Discriminitor" class="headerlink" title="Discriminitor"></a>Discriminitor</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">class FCDiscriminator(nn.Module):</span><br><span class="line">	def __init__(self, num_classes, ndf = 64):</span><br><span class="line">		super(FCDiscriminator, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)</span><br><span class="line"></span><br><span class="line">		self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)</span><br><span class="line">		#self.up_sample = nn.Upsample(scale_factor=32, mode=&apos;bilinear&apos;)</span><br><span class="line">		#self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">	def forward(self, x):</span><br><span class="line">		x = self.conv1(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv2(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv3(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv4(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.classifier(x)</span><br><span class="line">		#x = self.up_sample(x)</span><br><span class="line">		#x = self.sigmoid(x) </span><br><span class="line"></span><br><span class="line">		return x</span><br></pre></td></tr></table></figure>
<p>有了上面的描述，判别器的网络还是很清楚的。</p>
<h2 id="Segmentation-Network-1"><a href="#Segmentation-Network-1" class="headerlink" title="Segmentation Network"></a>Segmentation Network</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class ResNetMulti(nn.Module):</span><br><span class="line">    def __init__(self, block, layers, num_classes):</span><br><span class="line">        self.inplanes = 64</span><br><span class="line">        super(ResNetMulti, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,</span><br><span class="line">                               bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)</span><br><span class="line">        for i in self.bn1.parameters():</span><br><span class="line">            i.requires_grad = False</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change</span><br><span class="line">        self.layer1 = self._make_layer(block, 64, layers[0])</span><br><span class="line">        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)</span><br><span class="line">        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)</span><br><span class="line">        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)</span><br><span class="line">        self.layer5 = self._make_pred_layer(Classifier_Module, 1024, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)</span><br><span class="line">        self.layer6 = self._make_pred_layer(Classifier_Module, 2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)</span><br></pre></td></tr></table></figure>
<p>前面应该是对resnet的结构的继承吧，后面的layer5，和layer6应该就是前面说的ASPP的classifier了，这两个分别之后参与adaptation module的部分。</p>
<h1 id="Train-Code"><a href="#Train-Code" class="headerlink" title="Train Code"></a>Train Code</h1><h2 id="Train-G"><a href="#Train-G" class="headerlink" title="Train G"></a>Train G</h2><p>类似train 原本的GAN，这里train的G其实就是segmentation network</p>
<h3 id="Train-with-source"><a href="#Train-with-source" class="headerlink" title="Train with source"></a>Train with source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">_, batch = trainloader_iter.next()</span><br><span class="line">images, labels, _, _ = batch</span><br><span class="line">images = Variable(images).cuda(args.gpu)</span><br><span class="line"></span><br><span class="line">pred1, pred2 = model(images)</span><br><span class="line">pred1 = interp(pred1)</span><br><span class="line">pred2 = interp(pred2)</span><br><span class="line"></span><br><span class="line">loss_seg1 = loss_calc(pred1, labels, args.gpu)</span><br><span class="line">loss_seg2 = loss_calc(pred2, labels, args.gpu)</span><br><span class="line">loss = loss_seg2 + args.lambda_seg * loss_seg1</span><br><span class="line"></span><br><span class="line"># proper normalization</span><br><span class="line">loss = loss / args.iter_size</span><br><span class="line">loss.backward()</span><br><span class="line">loss_seg_value1 += loss_seg1.data.cpu().numpy()[0] / args.iter_size</span><br><span class="line">loss_seg_value2 += loss_seg2.data.cpu().numpy()[0] / args.iter_size</span><br></pre></td></tr></table></figure>
<p>train with source这里的loss就是：$\sum_i \lambda_i^{seg}L^i_{seg}(I_s)$，$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</p>
<h3 id="Train-with-target"><a href="#Train-with-target" class="headerlink" title="Train with target"></a>Train with target</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">_, batch = targetloader_iter.next()</span><br><span class="line">images, _, _ = batch</span><br><span class="line">images = Variable(images).cuda(args.gpu)</span><br><span class="line"></span><br><span class="line">pred_target1, pred_target2 = model(images)</span><br><span class="line">pred_target1 = interp_target(pred_target1)</span><br><span class="line">pred_target2 = interp_target(pred_target2)</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred_target1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred_target2))</span><br><span class="line"></span><br><span class="line">loss_adv_target1 = bce_loss(D_out1,</span><br><span class="line">                            Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(</span><br><span class="line">                                args.gpu))</span><br><span class="line"></span><br><span class="line">loss_adv_target2 = bce_loss(D_out2,</span><br><span class="line">                            Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(</span><br><span class="line">                                args.gpu))</span><br><span class="line"></span><br><span class="line">loss = args.lambda_adv_target1 * loss_adv_target1 + args.lambda_adv_target2 * loss_adv_target2</span><br><span class="line">loss = loss / args.iter_size</span><br><span class="line">loss.backward()</span><br><span class="line">loss_adv_target_value1 += loss_adv_target1.data.cpu().numpy()[0] / args.iter_size</span><br><span class="line">loss_adv_target_value2 += loss_adv_target2.data.cpu().numpy()[0] / args.iter_size</span><br></pre></td></tr></table></figure>
<p>train target对应的就是：$\sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$</p>
<h2 id="Train-D"><a href="#Train-D" class="headerlink" title="Train D"></a>Train D</h2><h3 id="Train-with-source-1"><a href="#Train-with-source-1" class="headerlink" title="Train with source"></a>Train with source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># labels for adversarial training</span><br><span class="line">source_label = 0</span><br><span class="line"></span><br><span class="line">pred1 = pred1.detach()</span><br><span class="line">pred2 = pred2.detach()</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred2))</span><br><span class="line"></span><br><span class="line">loss_D1 = bce_loss(D_out1,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D2 = bce_loss(D_out2,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D1 = loss_D1 / args.iter_size / 2</span><br><span class="line">loss_D2 = loss_D2 / args.iter_size / 2</span><br><span class="line"></span><br><span class="line">loss_D1.backward()</span><br><span class="line">loss_D2.backward()</span><br><span class="line"></span><br><span class="line">loss_D_value1 += loss_D1.data.cpu().numpy()</span><br><span class="line">loss_D_value2 += loss_D2.data.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，z = 0</p>
<h3 id="Train-with-target-1"><a href="#Train-with-target-1" class="headerlink" title="Train with target"></a>Train with target</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># labels for adversarial training</span><br><span class="line">target_label = 1</span><br><span class="line"></span><br><span class="line">pred_target1 = pred_target1.detach()</span><br><span class="line">pred_target2 = pred_target2.detach()</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred_target1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred_target2))</span><br><span class="line"></span><br><span class="line">loss_D1 = bce_loss(D_out1,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out1.data.size()).fill_(target_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D2 = bce_loss(D_out2,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out2.data.size()).fill_(target_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D1 = loss_D1 / args.iter_size / 2</span><br><span class="line">loss_D2 = loss_D2 / args.iter_size / 2</span><br><span class="line"></span><br><span class="line">loss_D1.backward()</span><br><span class="line">loss_D2.backward()</span><br><span class="line"></span><br><span class="line">loss_D_value1 += loss_D1.data.cpu().numpy()</span><br><span class="line">loss_D_value2 += loss_D2.data.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，z = 1</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/leetcode体系/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/leetcode体系/" itemprop="url">leetcode体系</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-20T22:39:09-04:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>leetcode里面考察很多的算法知识：</p>
<ol>
<li>基本的数据结构：<br> queue, stack, heap, tree, graph，linked-list，trie</li>
<li>一些基本的算法：<br> sort<br> binary search</li>
<li>一些高级点的算法：<br> dp，bfs，dfs</li>
<li>别忘了位运算，和‘2’有关的时候很有用，可以简化问题<br>边刷，边整理和总结</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/GAN-pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/GAN-pytorch/" itemprop="url">GAN@pytorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-20T15:45:51-04:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>莫烦python也给了pytorch的GAN<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/4-06-GAN/" target="_blank" rel="noopener">版本</a>：<br>然后莫烦的全部代码在<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py" target="_blank" rel="noopener">这里</a></p>
<h1 id="hyper-parameter"><a href="#hyper-parameter" class="headerlink" title="hyper parameter"></a>hyper parameter</h1><p>新手画家 (Generator) 在作画的时候需要有一些灵感 (random noise), 我们这些灵感的个数定义为 N_IDEAS. 而一幅画需要有一些规格, 我们将这幅画的画笔数定义一下, N_COMPONENTS 就是一条一元二次曲线(这幅画画)上的点个数. 为了进行批训练, 我们将一整批话的点都规定一下(PAINT_POINTS).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(1) # reproducible</span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line"># hyper parameter</span><br><span class="line">BATCH_SIZE = 64</span><br><span class="line">LR_G = 0.0001 </span><br><span class="line">LR_D = 0.0001</span><br><span class="line">N_IDEAS = 5</span><br><span class="line">ART_COMPONENTS = 15</span><br><span class="line">PAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])</span><br></pre></td></tr></table></figure></p>
<h1 id="著名画家的画"><a href="#著名画家的画" class="headerlink" title="著名画家的画"></a>著名画家的画</h1><p>我们需要有很多画是来自著名画家的(real data), 将这些著名画家的画, 和新手画家的画都传给新手鉴赏家, 让鉴赏家来区分哪些是著名画家, 哪些是新手画家的画. 如何区分我们在后面呈现. 这里我们生成一些著名画家的画 (batch 条不同的一元二次方程曲线).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def artist_works(): # real target</span><br><span class="line">    a = np.random.uniform(1, 2, size = BATCH_SIZE)[:, np.newaxis]</span><br><span class="line">    paintings = a * np.power(PAINT_POINTS, 2) + (a - 1)</span><br><span class="line">    paintings = torch.from_numpy(paintings).float()</span><br><span class="line">    return paintings</span><br></pre></td></tr></table></figure>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>这里会创建两个神经网络, 分别是 Generator (新手画家), Discriminator(新手鉴赏家). G 会拿着自己的一些灵感当做输入, 输出一元二次曲线上的点 (G 的画).</p>
<p>D 会接收一幅画作 (一元二次曲线), 输出这幅画作到底是不是著名画家的画(是著名画家的画的概率).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">G = nn.Sequential(                      # Generator</span><br><span class="line">    nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">D = nn.Sequential(                      # Discriminator</span><br><span class="line">    nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(128, 1),</span><br><span class="line">    nn.Sigmoid(),                       # tell the probability that the art work is made by artist</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>接着我们来同时训练 D 和 G. 训练之前, 我们来看看G作画的原理. G 首先会有些灵感, G_ideas 就会拿到这些随机灵感 (可以是正态分布的随机数), 然后 G 会根据这些灵感画画. 接着我们拿着著名画家的画和 G 的画, 让 D 来判定这两批画作是著名画家画的概率.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for step in range(10000):</span><br><span class="line">    artist_paintings = artist_works()           # real painting from artist</span><br><span class="line">    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)    # random ideas</span><br><span class="line">    G_paintings = G(G_ideas())                  # fake painting from G (random ideas)</span><br><span class="line"></span><br><span class="line">    prob_artist0 = D(artist_paintings)          # D try to increase this prob</span><br><span class="line">    prob_artist1 = D(G_paintings)               # D try to reduce this prob</span><br></pre></td></tr></table></figure>
<p>然后计算有多少来之画家的画猜对了, 有多少来自 G 的画猜对了, 我们想最大化这些猜对的次数. 这也就是 log(D(x)) + log(1-D(G(z)) 在论文中的形式. 而因为 torch 中提升参数的形式是最小化误差, 那我们把最大化 score 转换成最小化 loss, 在两个 score 的合的地方加一个符号就好. 而 G 的提升就是要减小 D 猜测 G 生成数据的正确率, 也就是减小 D_score1.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">D_loss = -torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))</span><br><span class="line">G_loss = torch.mean(torch.log(1. - prob_artist1))</span><br></pre></td></tr></table></figure>
<p>最后我们在根据 loss 提升神经网络就好了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">opt_D.zero_grad()</span><br><span class="line">D_loss.backward(retain_graph=True)              # retain_graph 这个参数是为了再次使用计算图纸</span><br><span class="line">opt_D.step()</span><br><span class="line"></span><br><span class="line">opt_G.zero_grad()</span><br><span class="line">G_loss.backward()</span><br><span class="line">opt_G.step()</span><br></pre></td></tr></table></figure></p>
<p>之后就是，开始整理总结最近在看的那篇论文作者代码的解读了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/pytorch/" itemprop="url">pytorch 入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-20T15:30:49-04:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tech/" itemprop="url" rel="index">
                    <span itemprop="name">tech</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近在看domain adaptation的代码，需要学习pytorch，然后参考<a href="https://morvanzhou.github.io/" target="_blank" rel="noopener">莫烦python的代码</a>，自己整理一遍:</p>
<h1 id="用-Numpy-还是-Torch"><a href="#用-Numpy-还是-Torch" class="headerlink" title="用 Numpy 还是 Torch"></a>用 Numpy 还是 Torch</h1><p>Torch 自称为神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是用 Torch 的 tensor 形式数据最好咯. 就像 Tensorflow 当中的 tensor 一样.</p>
<p>当然, 我们对 Numpy 还是爱不释手的, 因为我们太习惯 numpy 的形式了. 不过 torch 看出来我们的喜爱, 他把 torch 做的和 numpy 能很好的兼容. 比如这样就能自由地转换 numpy array 和 torch tensor 了:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(6).reshape((2, 3))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line">print(</span><br><span class="line">    &apos;\nnumpy array:&apos;, np_data,          # [[0 1 2], [3 4 5]]</span><br><span class="line">    &apos;\ntorch tensor:&apos;, torch_data,      # 0 1 2 \n 3 4 5 [torch.LongTensor of 2 x 3]</span><br><span class="line">    &apos;\ntensor to array&apos;: tensor2array,  # [[0 1 2], [3 4 5]]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="Torch-中的数学运算"><a href="#Torch-中的数学运算" class="headerlink" title="Torch 中的数学运算"></a>Torch 中的数学运算</h1><h2 id="简单运算"><a href="#简单运算" class="headerlink" title="简单运算"></a>简单运算</h2><p>其实 torch 中 tensor 的运算和 numpy array 的如出一辙, 我们就以对比的形式来看. 如果想了解 torch 中其它更多有用的运算符，<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">API就是你要去的地方</a>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># abs 绝对值的计算</span><br><span class="line">data = [-1, -2, 1, 2]</span><br><span class="line">tensor = torch.FloatTensor(data)</span><br><span class="line">print(</span><br><span class="line">    &apos;\nabs&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.abs(data),          # [1 2 1 2]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.abs(tensor)      # [1 2 1 2]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># sin   三角函数 sin</span><br><span class="line">print(</span><br><span class="line">    &apos;\nsin&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># mean  均值</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmean&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.mean(data),         # 0.0</span><br><span class="line">    &apos;\ntorch: &apos;, torch.mean(tensor)     # 0.0</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h2><p>除了简单的计算, 矩阵运算才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># matrix multiplication 矩阵点乘</span><br><span class="line">data = [[1,2], [3,4]]</span><br><span class="line">tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor</span><br><span class="line"># correct method</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmatrix multiplication (matmul)&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.matmul(data, data),     # [[7, 10], [15, 22]]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># !!!!  下面是错误的方法 !!!!</span><br><span class="line">data = np.array(data)</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmatrix multiplication (dot)&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, data.dot(data),        # [[7, 10], [15, 22]] 在numpy 中可行</span><br><span class="line">    &apos;\ntorch: &apos;, tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>新版本中(&gt;=0.3.0), 关于 tensor.dot() 有了新的改变, 它只能针对于一维的数组. 所以上面的有所改变.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span><br><span class="line"></span><br><span class="line"># 变为</span><br><span class="line">torch.dot(tensor.dot(tensor)</span><br></pre></td></tr></table></figure></p>
<h1 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h1><h2 id="什么是Variable"><a href="#什么是Variable" class="headerlink" title="什么是Variable"></a>什么是Variable</h2><p>这个感觉和tensorflow里面的一样，就是存有变化的数值的地方，然后这个变化的值就是tensor。然后，这里Variable可能有点像tensorflow里面的placeholder吧</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor(([1, 2], [3, 4]))</span><br><span class="line"># requires_grad 是参不参与误差的反向传播，要不要计算梯度</span><br><span class="line">variable = Variable(tensor, requires_grad=True)</span><br><span class="line"></span><br><span class="line">print(tensor)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Variable containing:</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<h2 id="Variable-计算，梯度"><a href="#Variable-计算，梯度" class="headerlink" title="Variable 计算，梯度"></a>Variable 计算，梯度</h2><p>我们再对比一下 tensor 的计算和 variable 的计算.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       # x^2</span><br><span class="line">v_out = torch.mean(variable*variable)   # x^2</span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)    # 7.5</span><br></pre></td></tr></table></figure></p>
<p>这里我们应该也看不出Variable和一般tensor的不同，和tensorflow类似，Variable参与计算时，也是在打一个computational graph （原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦，毕竟，tensor 只是一个值而已。）</p>
<p><strong>v_out = torch.mean(variable*variable)</strong> 就是在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳, 我们就来举个例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">v_out.backward()    # 模拟 v_out 的误差反向传递</span><br><span class="line"></span><br><span class="line"># 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.</span><br><span class="line"># v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span><br><span class="line"># 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span><br><span class="line"></span><br><span class="line">print(variable.grad)    # 初始 Variable 的梯度</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"> 0.5000  1.0000</span><br><span class="line"> 1.5000  2.0000</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<h2 id="获取-Variable-里面的数据"><a href="#获取-Variable-里面的数据" class="headerlink" title="获取 Variable 里面的数据"></a>获取 Variable 里面的数据</h2><p>直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图), 所以我们要转换一下, 将它变成 tensor 形式.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     #  Variable 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Variable containing:</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable.data)    # tensor 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable.data.numpy())    # numpy 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">[[ 1.  2.]</span><br><span class="line"> [ 3.  4.]]</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></p>
<h1 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h1><h2 id="什么是-Activation"><a href="#什么是-Activation" class="headerlink" title="什么是 Activation"></a>什么是 Activation</h2><p>为什么需要非线性的函数？</p>
<ul>
<li>线性的话，你多少层都一样</li>
<li>非线性的话，可以拟合不同的函数</li>
</ul>
<h2 id="Torch-中的激励函数"><a href="#Torch-中的激励函数" class="headerlink" title="Torch 中的激励函数"></a>Torch 中的激励函数</h2><p>Torch 中的激励函数有很多, 不过我们平时要用到的就这几个. relu, sigmoid, tanh, softplus. 那我们就看看他们各自长什么样啦.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F     # 激励函数都在这</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line"># 做一些假数据来观看图像</span><br><span class="line">x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)</span><br><span class="line">x = Variable(x)</span><br></pre></td></tr></table></figure>
<p>接着就是做生成不同的激励函数数据:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x_np = x.data.numpy()   # 换成 numpy array, 出图时用</span><br><span class="line"></span><br><span class="line"># 几种常用的 激励函数</span><br><span class="line">y_relu = F.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = F.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = F.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</span><br></pre></td></tr></table></figure></p>
<p>接着我们开始画图, 画图的代码也在下面:<br><img src="/images/activationfunc.jpeg" width="50%" height="50%"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt  # python 的可视化模块, 我有教程 (https://morvanzhou.github.io/tutorials/data-manipulation/plt/)</span><br><span class="line"></span><br><span class="line">plt.figure(1, figsize=(8, 6))</span><br><span class="line">plt.subplot(221)</span><br><span class="line">plt.plot(x_np, y_relu, c=&apos;red&apos;, label=&apos;relu&apos;)</span><br><span class="line">plt.ylim((-1, 5))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.subplot(222)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=&apos;red&apos;, label=&apos;sigmoid&apos;)</span><br><span class="line">plt.ylim((-0.2, 1.2))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.subplot(223)</span><br><span class="line">plt.plot(x_np, y_tanh, c=&apos;red&apos;, label=&apos;tanh&apos;)</span><br><span class="line">plt.ylim((-1.2, 1.2))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.subplot(224)</span><br><span class="line">plt.plot(x_np, y_softplus, c=&apos;red&apos;, label=&apos;softplus&apos;)</span><br><span class="line">plt.ylim((-0.2, 6))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="搭神经网络"><a href="#搭神经网络" class="headerlink" title="搭神经网络"></a>搭神经网络</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/17/深度学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/17/深度学习/" itemprop="url">深度学习篇</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-17T21:56:35-04:00">
                2018-06-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>虽然，强化学习、深度学习和迁移学习都是属于机器学习的，但是感觉blog里面，放在一起还是有点杂乱，所以还是分出来，慢慢整理，<br>我应该主要整理cmu <a href="https://www.cs.cmu.edu/~rsalakhu/10707/" target="_blank" rel="noopener">10707</a> introduction to deep learning的东西，也会参考<a href="http://deeplearning.cs.cmu.edu/" target="_blank" rel="noopener">11785</a> introduction to deep learning</p>
<p>就是用自己的理解整理一遍吧，之前面试总是面挂在这里，希望之后不会，还有对自己目前research也有点帮助吧。毕竟基础不牢，地动山摇。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/17/DNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/17/DNN/" itemprop="url">DNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-17T21:36:58-04:00">
                2018-06-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这个是hugo的deep learning的学习<a href="http://www.dmi.usherb.ca/~larocheh/neural_networks/content.html" target="_blank" rel="noopener">tutorial</a>，需要慢慢刷掉，当然cmu 10707 的<a href="https://www.cs.cmu.edu/~rsalakhu/10707/" target="_blank" rel="noopener">slides</a>很多也是参考这个的(Russlan自己说)整理的，你如果找不到CMU 10707的视频你可以看这个，也是极好的。</p>
<p>然后，这里我想整理下最最基本的neural network的公式的推导，自己再推一遍，感觉就不虚：</p>
<h1 id="网络的基本结构："><a href="#网络的基本结构：" class="headerlink" title="网络的基本结构："></a>网络的基本结构：</h1><p><img src="/images/DNNstructure.jpeg" width="50%" height="50%"></p>
<ul>
<li>layer pre-activation for $k &gt; 0$ <strong>$(h_{0}(x) = x)$</strong><br>  $a^{(k)}(x) = b^{(k)} + W^{(k)}h^{(k - 1)}(x)$<br>  感觉这么记忆，不会记乱，k就是第几层neuron，然后$h^{(k - 1)}(x)$是前一层的输出，然后weights $W^{(k)}$，bias $b^{(k)}$都是这层的，虽然有前一层输出作为的输入，但是这些还是算这一层的。</li>
<li><p>hidden layer activation (k from 1 to L)<br>  $h^{(k)}(x) = g(a^{(k)}(x))$<br>  反正这个就是这一层的输出，然后，这层的weights，bias最终都会由这层的输出所终结。</p>
</li>
<li><p>output layer activation $k = L + 1$:<br>  $h^{(L + 1)}(x) = o(a^{(L + 1)}(x)) = f(x)$<br>  和hidden不一样就是这个是最后一层了，然后这层的activation function也会和之前hidden layers有所不同，如果是分类的话，往往是softmax</p>
</li>
<li><p>softmax activation function at the output:<br>  $o(a) = softmax(a) = [\frac{exp(a_1)}{\sum_c exp(a_c)}…\frac{a_C}{\sum_c exp(a_c)}]^T$<br>  如何理解softmax呢，为什么多分类问题里面要用softmax呢，而不是别的呢？<br>  知乎对此有一定的<a href="https://www.zhihu.com/question/40403377" target="_blank" rel="noopener">讨论</a>，我这里引用下王赟(yun第一声)大神的解答：</p>
<ul>
<li>原因之一：希望特征对概率的影响是乘性的</li>
<li>原因之二：多类分类问题的目标函数常常选为cross-entropy，…(推完整个，回来补)</li>
</ul>
</li>
<li><p>activation function:</p>
<ul>
<li><p>sigmoid：</p>
<ul>
<li>formula:<br>  $\sigma(x) = \frac{1}{1 + e^{-x}}$，$\sigma(x)’ = \frac{e^x}{(1 + e^x)^2} = (1 - \sigma(x)) \sigma(x)$</li>
<li>shortcomings:<ul>
<li>gradient vanish</li>
<li>symmetric</li>
<li>time cosuming to compute exp</li>
</ul>
</li>
</ul>
</li>
<li><p>tanh:</p>
<ul>
<li>formula:<br>  $tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1 + e^{-2}} - 1 = 2 \sigma(2x) - 1$，$tanh(x)’ = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</li>
<li>感觉就是解决了原点对称的问题</li>
</ul>
</li>
<li><p>relu:</p>
<ul>
<li>formula：$f(x) = max(0, x)$</li>
<li>优点：<br>  Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生<br>  计算量小</li>
<li>缺点：<br>  部分neuro会死亡</li>
</ul>
</li>
<li><p>leaky relu：</p>
<ul>
<li>formula：$f(x) = max(\epsilon x, x)$</li>
<li>优点:<br>  解决了neuron会死亡的问题</li>
</ul>
</li>
<li><p>maxout：</p>
<ul>
<li>formula: 对 relu 和 leaky relu的一般归纳：$f(x) = max(w_1^T x + b_1, w_2^T x + b_2)$</li>
<li>优点:<br>  计算简单，不会死亡，不会饱和</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function:"></a>loss function:</h1><ul>
<li><p>stochastic gradient descent (SGD):<br>  随机梯度下降应该是最最基础的梯度下降的方法了，</p>
<ul>
<li>initialize  $\theta$ ($\theta = {W^{(1)}, b^{(1)}…，W^{(L + 1)}}$)</li>
<li>algorithm:<br>  for N iterations: (One epoch)<pre><code>for each training example $(x_{(t)}, y_{(t)})$   
    $\delta = -\nabla_{\theta}l(f(x_{(t)}, \theta), \y_{(t)}) - \lambda\nabla_{(\theta)}$
    $\omiga_{(\theta)}$
    $\theta \leftarrow \theta + \alpha \delta$
</code></pre></li>
<li>SGD 的<a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">优缺点</a>：<ul>
<li>缺点：<ul>
<li>选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了</li>
<li>相对BGD noise会比较大</li>
</ul>
</li>
</ul>
</li>
<li><p>batch gradient descent (BGD) 的<a href="https://zhuanlan.zhihu.com/p/25765735" target="_blank" rel="noopener">对比</a>：<br>  所谓batch就是一起算，你看公式就知道：<br>  $\theta \leftarrow \theta + \frac{1}{m}\sum_{i}(y_i - f(x;\theta)(x_i))$ (MSE)</p>
<ul>
<li>缺点：m很大的时候，train的会比较慢</li>
<li>优点：比SGD稳定</li>
</ul>
</li>
<li><p>mini-batch GD:<br>  就是这两个的折中，就像强化学习里面的，TD，Monta Carlo之间的n step-TD</p>
<ul>
<li>advantages:<ul>
<li>give a accurate estimate of average loss</li>
<li>can leverage matrix operations, which cost less than BGD</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>what neural network estimates?<br>  $f(x)_{c} = P(y=c|x)$， where c means which class.</p>
</li>
<li><p>what to optimize?<br>  maximize log likelihood —- minize negative log likelihood: $P(y_i=c|x_i)$，given $(x_i, y_i)$<br>  cross-entropy: p, q (p one-hot, q distribution of the P(y=c|x))<br>  $l(f(x), y) = -\sum_c1(y=c)log f(x)_c = - log f(x)_y$</p>
</li>
</ul>
<h1 id="loss-gradient-output"><a href="#loss-gradient-output" class="headerlink" title="loss gradient output:"></a>loss gradient output:</h1><p><img src="/images/DNN-output-layer-gradient.jpeg" width="50%" height="50%"></p>
<h2 id="loss-gradient-at-output"><a href="#loss-gradient-at-output" class="headerlink" title="loss gradient at output"></a>loss gradient at output</h2><ul>
<li><p>partial derivative:<br>  $\frac{\partial - logf(x)_y}{\partial f(x)_c} = \frac{-1^{(y = c)}}{f(x)^y}$<br>  这里，y要和c一样才有值，因为这里cross-entropy里面用了one-hot，只有在同一维度下面，求偏导才有值。</p>
</li>
<li><p>gradient:<br>  然后，我们推广到，求梯度<br>  $\nabla_{f(x)} -logf(x)_y= \frac{-1}{f(x)_y} [1^{(y=0)}…1^{(y=C-1)}]^T = \frac{-e(y)}{f(x)^y}$</p>
</li>
</ul>
<h2 id="loss-gradient-at-output-pre-activation"><a href="#loss-gradient-at-output-pre-activation" class="headerlink" title="loss gradient at output pre-activation"></a>loss gradient at output pre-activation</h2><ul>
<li><p>partial derivative:<br>  首先还是标量的形式，<br>  $\frac{\partial - logf(x)_y}{\partial a^{(L+1)}(x)_c} = (1^{(y = c)}} - f(x)^y)$<br>  这里，y要和c一样才有值，因为这里cross-entropy里面用了one-hot，只有在同一维度下面，求偏导才有值。</p>
</li>
<li><p>gradient:<br>  然后，我们类比到向量上面，<br>  $\nabla_{a^{(L+1)}(x)_c}[- logf(x)_y}] = -(e^{(y)}} - f(x)^y)$</p>
</li>
<li><p>proof:<br>  这里的proof不完整，只是推了一个维度的，完整的可以参考ece一个师兄的知乎的<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a>，之后回来在补推一下。<br><img src="/images/DNN-output-layer-gradient-proof.jpeg" width="70%" height="70%"></p>
</li>
</ul>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><h2 id="Compute-output-gradient-before-activation"><a href="#Compute-output-gradient-before-activation" class="headerlink" title="Compute output gradient (before activation)"></a>Compute output gradient (before activation)</h2><p>$\nabla_{a^{(L+1)}(x)} -logf(x)_y \leftarrow - (e(y)-f(x))$</p>
<h2 id="for-k-from-L-1-to-1"><a href="#for-k-from-L-1-to-1" class="headerlink" title="for k from L + 1 to 1"></a>for k from L + 1 to 1</h2><ul>
<li>compute gradients of hidden layer parameter<br>$\nabla_{W^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y h^{(k-1)}(x)^T$<br>$\nabla_{b^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below<br>$\nabla_{b^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below<br>$\nabla_{h^{(k-1)}(x)} -logf(x)^y \leftarrow $ $W^{(k)T} \nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below (before activation)<br>$\nabla_{a^{(k-1)}(x)} -logf(x)^y \leftarrow $ $(\nabla_{h^{(k-1)}(x)} -log f(x)^y) \odot […,g’(a^{(k-1)}(x)_j),…]$</li>
</ul>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><ul>
<li><p>L1 &amp; L2 regularization</p>
<ul>
<li>L1 $\frac{\lambda}{2m} \sum |w|^2$<br>  L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).</li>
<li>L2 $\frac{\lambda}{2m} \sum |w|$<br>  Unlike L2, the weights may be reduced to zero here. Hence, it is very useful when we are trying to compress our model. Otherwise, we usually prefer L2 over it. Sparse solution.</li>
</ul>
</li>
<li><p>Dropout<br>  So what does dropout do? At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections as shown below.<br>  <img src="/images/dropout.jpeg" width="70%" height="70%"><br>  So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning.</p>
<p>  Ensemble models usually perform better than a single model as they capture more randomness. Similarly, dropout also performs better than a normal neural network model.</p>
<p>  This probability of choosing how many nodes should be dropped is the hyperparameter of the dropout function. As seen in the image above, dropout can be applied to both the hidden layers as well as the input layers.</p>
</li>
<li><p>Batch Normalization</p>
<ul>
<li>idea<br>  is that since it’s benefit to training if the input data is normalized, so why not normalize in hidden layers to solve the internal covariance shift.</li>
<li>denormalization<br>  to avoid extra effect of normalization, the denormalization parameters are helpful to adjust<br><img src="/images/batch-normalization.jpeg" width="70%" height="70%"></li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chu Lin</p>
              <p class="site-description motion-element" itemprop="description">去人迹罕至的地方，留下自己的足迹。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">73</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chu Lin</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
