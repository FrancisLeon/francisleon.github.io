<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="GAN,pytorch," />










<meta name="description" content="接下来我开始解读，这篇论文的code Network Structure and TrainingDiscriminatorFor the discriminator, we use an architecture similar to but utilize all fully-convolutional lay- ers to retain the spatial information.总是">
<meta name="keywords" content="GAN,pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="multi-level-GAN-code解读">
<meta property="og:url" content="http://yoursite.com/2018/06/24/multi-level-GAN-code解读/index.html">
<meta property="og:site_name" content="Truly">
<meta property="og:description" content="接下来我开始解读，这篇论文的code Network Structure and TrainingDiscriminatorFor the discriminator, we use an architecture similar to but utilize all fully-convolutional lay- ers to retain the spatial information.总是">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/multi-GAN-structure.jpeg">
<meta property="og:updated_time" content="2018-07-04T04:14:46.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="multi-level-GAN-code解读">
<meta name="twitter:description" content="接下来我开始解读，这篇论文的code Network Structure and TrainingDiscriminatorFor the discriminator, we use an architecture similar to but utilize all fully-convolutional lay- ers to retain the spatial information.总是">
<meta name="twitter:image" content="http://yoursite.com/images/multi-GAN-structure.jpeg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/24/multi-level-GAN-code解读/"/>





  <title>multi-level-GAN-code解读 | Truly</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Truly</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/24/multi-level-GAN-code解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">multi-level-GAN-code解读</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-24T11:24:59-04:00">
                2018-06-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>接下来我开始解读，<a href="https://arxiv.org/abs/1802.10349" target="_blank" rel="noopener">这篇</a>论文的<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">code</a></p>
<h1 id="Network-Structure-and-Training"><a href="#Network-Structure-and-Training" class="headerlink" title="Network Structure and Training"></a>Network Structure and Training</h1><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>For the discriminator, we use an architecture similar to but utilize all fully-convolutional lay- ers to <strong>retain the spatial information</strong>.<br>总是来说就是，5层卷积网络，kernel是4 x 4 stride是2，channel是{64, 128, 256, 512, 1}。<br>除了最后一层卷积层，其他所有层都是用参数为0.2的leaky ReLU。在最后一层卷积层之后加了一个up-sampling的层，使得最后一层和输入图片的大小是一样的。他们没有使用batch-normalization层，因为他们用小的batch size一起训练判别器和分割网络。(?)</p>
<ul>
<li>batch normalization:</li>
</ul>
<h2 id="Segmentation-Network"><a href="#Segmentation-Network" class="headerlink" title="Segmentation Network"></a>Segmentation Network</h2><p>他们用DeepLab-v2 和 ResNet-101来作为他们分割的baseline，由于memory的问题，他们没有使用multi-scale。<br>他们去掉了最后的分类的一层，然后将最后的两层卷积stride从2改成1。这使得输出的feature maps是输入图片大小的1/8。为了使这个更大，他们在conv4和conv5用了stride分别是2，4的dilated conv。这后面又用了Atrous Spatial Pyramid Pooling (ASPP)作为最后的分类器。在ASPP后面，他们也采用了输出的是softmax的up-sampling层，这层输出的大小和输入的图片大小也是一样的。</p>
<ul>
<li>ASPP:</li>
</ul>
<h2 id="Multi-level-Adaptation-Model"><a href="#Multi-level-Adaptation-Model" class="headerlink" title="Multi-level Adaptation Model"></a>Multi-level Adaptation Model</h2><p>上面的构成了他们single-level的网络结构。为了构建multi-level的结构，他们将conv4的feature map和一个作为辅助分类器的ASPP模块相结合。和single-level类似，这里面也加了一个同样结构的判别器来进行对抗学习。如图：<br><img src="/images/multi-GAN-structure.jpeg" width="70%" height="70%"></p>
<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>作者发现，将segmentation network和discriminitor一起训练效率会比较高。<br>对源域将图片$I_s$向前传最后得到$P_s$，以及优化$L_{seg}$。对于目标域，我们将得到的$P_t$和$P_s$一起输入到判别器里面，然后优化$L_{d}$。此外，对于$P_t$，我们还需要计算对抗损失$L_{ad}$。</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul>
<li><p>whole objective:<br>  $L(I_s, I_t) = L_{seg}(I_s) + \lambda L_{adv}(I_t)$</p>
<ul>
<li>$L_{seg}(I_s)$<br>  cross-entropy loss using ground truth annotations in the source domain</li>
<li>$L_{adv}$<br>  对抗损失，用来使得源域的预期的数据分布和目标域相近</li>
<li>$\lambda_{abv}$<br>  这个weight用来平衡这两个loss</li>
</ul>
</li>
<li><p>discriminitor:</p>
<ul>
<li><p>segmentation softmax output:<br>  $P = G(I) \in R^{HxWxC}$, 这里C是种类数，这里C是19</p>
</li>
<li><p>cross-entropy loss：<br>  我们将P传到全卷积的判别器D里面：$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，这个是binary cross entropy，这里z = 0，表示来自target，z = 1表示来自source</p>
</li>
</ul>
</li>
<li><p>segmentation network:</p>
<ul>
<li>segmentation loss:<br>  在源域的话我们正常训练，还是由cross-entropy loss来定义：$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</li>
<li>adversarial loss：<br>  在目标域，我们的对抗损失是：$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$，这个损失是用来欺骗判别器的，使得两者的预期的概率的一致</li>
</ul>
</li>
<li><p>multi-level:</p>
<ul>
<li>multi-level loss<br>  就是在low-level的feature space里面加上上面的loss，也不是很难理解：<br>  $L_{I_s, I_t} = \sum_i \lambda_i^{seg}L^i_{seg}(I_s) + \sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，i表示第几层网络。</li>
</ul>
</li>
</ul>
<h1 id="Network-Code"><a href="#Network-Code" class="headerlink" title="Network Code"></a>Network Code</h1><h2 id="Discriminitor"><a href="#Discriminitor" class="headerlink" title="Discriminitor"></a>Discriminitor</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">class FCDiscriminator(nn.Module):</span><br><span class="line">	def __init__(self, num_classes, ndf = 64):</span><br><span class="line">		super(FCDiscriminator, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)</span><br><span class="line"></span><br><span class="line">		self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)</span><br><span class="line">		#self.up_sample = nn.Upsample(scale_factor=32, mode=&apos;bilinear&apos;)</span><br><span class="line">		#self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">	def forward(self, x):</span><br><span class="line">		x = self.conv1(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv2(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv3(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv4(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.classifier(x)</span><br><span class="line">		#x = self.up_sample(x)</span><br><span class="line">		#x = self.sigmoid(x) </span><br><span class="line"></span><br><span class="line">		return x</span><br></pre></td></tr></table></figure>
<p>有了上面的描述，判别器的网络还是很清楚的。</p>
<h2 id="Segmentation-Network-1"><a href="#Segmentation-Network-1" class="headerlink" title="Segmentation Network"></a>Segmentation Network</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class ResNetMulti(nn.Module):</span><br><span class="line">    def __init__(self, block, layers, num_classes):</span><br><span class="line">        self.inplanes = 64</span><br><span class="line">        super(ResNetMulti, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,</span><br><span class="line">                               bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)</span><br><span class="line">        for i in self.bn1.parameters():</span><br><span class="line">            i.requires_grad = False</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change</span><br><span class="line">        self.layer1 = self._make_layer(block, 64, layers[0])</span><br><span class="line">        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)</span><br><span class="line">        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)</span><br><span class="line">        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)</span><br><span class="line">        self.layer5 = self._make_pred_layer(Classifier_Module, 1024, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)</span><br><span class="line">        self.layer6 = self._make_pred_layer(Classifier_Module, 2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)</span><br></pre></td></tr></table></figure>
<p>前面应该是对resnet的结构的继承吧，后面的layer5，和layer6应该就是前面说的ASPP的classifier了，这两个分别之后参与adaptation module的部分。</p>
<h1 id="Train-Code"><a href="#Train-Code" class="headerlink" title="Train Code"></a>Train Code</h1><h2 id="Train-G"><a href="#Train-G" class="headerlink" title="Train G"></a>Train G</h2><p>类似train 原本的GAN，这里train的G其实就是segmentation network</p>
<h3 id="Train-with-source"><a href="#Train-with-source" class="headerlink" title="Train with source"></a>Train with source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">_, batch = trainloader_iter.next()</span><br><span class="line">images, labels, _, _ = batch</span><br><span class="line">images = Variable(images).cuda(args.gpu)</span><br><span class="line"></span><br><span class="line">pred1, pred2 = model(images)</span><br><span class="line">pred1 = interp(pred1)</span><br><span class="line">pred2 = interp(pred2)</span><br><span class="line"></span><br><span class="line">loss_seg1 = loss_calc(pred1, labels, args.gpu)</span><br><span class="line">loss_seg2 = loss_calc(pred2, labels, args.gpu)</span><br><span class="line">loss = loss_seg2 + args.lambda_seg * loss_seg1</span><br><span class="line"></span><br><span class="line"># proper normalization</span><br><span class="line">loss = loss / args.iter_size</span><br><span class="line">loss.backward()</span><br><span class="line">loss_seg_value1 += loss_seg1.data.cpu().numpy()[0] / args.iter_size</span><br><span class="line">loss_seg_value2 += loss_seg2.data.cpu().numpy()[0] / args.iter_size</span><br></pre></td></tr></table></figure>
<p>train with source这里的loss就是：$\sum_i \lambda_i^{seg}L^i_{seg}(I_s)$，$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</p>
<h3 id="Train-with-target"><a href="#Train-with-target" class="headerlink" title="Train with target"></a>Train with target</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">_, batch = targetloader_iter.next()</span><br><span class="line">images, _, _ = batch</span><br><span class="line">images = Variable(images).cuda(args.gpu)</span><br><span class="line"></span><br><span class="line">pred_target1, pred_target2 = model(images)</span><br><span class="line">pred_target1 = interp_target(pred_target1)</span><br><span class="line">pred_target2 = interp_target(pred_target2)</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred_target1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred_target2))</span><br><span class="line"></span><br><span class="line">loss_adv_target1 = bce_loss(D_out1,</span><br><span class="line">                            Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(</span><br><span class="line">                                args.gpu))</span><br><span class="line"></span><br><span class="line">loss_adv_target2 = bce_loss(D_out2,</span><br><span class="line">                            Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(</span><br><span class="line">                                args.gpu))</span><br><span class="line"></span><br><span class="line">loss = args.lambda_adv_target1 * loss_adv_target1 + args.lambda_adv_target2 * loss_adv_target2</span><br><span class="line">loss = loss / args.iter_size</span><br><span class="line">loss.backward()</span><br><span class="line">loss_adv_target_value1 += loss_adv_target1.data.cpu().numpy()[0] / args.iter_size</span><br><span class="line">loss_adv_target_value2 += loss_adv_target2.data.cpu().numpy()[0] / args.iter_size</span><br></pre></td></tr></table></figure>
<p>train target对应的就是：$\sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$</p>
<h2 id="Train-D"><a href="#Train-D" class="headerlink" title="Train D"></a>Train D</h2><h3 id="Train-with-source-1"><a href="#Train-with-source-1" class="headerlink" title="Train with source"></a>Train with source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># labels for adversarial training</span><br><span class="line">source_label = 0</span><br><span class="line"></span><br><span class="line">pred1 = pred1.detach()</span><br><span class="line">pred2 = pred2.detach()</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred2))</span><br><span class="line"></span><br><span class="line">loss_D1 = bce_loss(D_out1,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D2 = bce_loss(D_out2,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D1 = loss_D1 / args.iter_size / 2</span><br><span class="line">loss_D2 = loss_D2 / args.iter_size / 2</span><br><span class="line"></span><br><span class="line">loss_D1.backward()</span><br><span class="line">loss_D2.backward()</span><br><span class="line"></span><br><span class="line">loss_D_value1 += loss_D1.data.cpu().numpy()</span><br><span class="line">loss_D_value2 += loss_D2.data.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，z = 0</p>
<h3 id="Train-with-target-1"><a href="#Train-with-target-1" class="headerlink" title="Train with target"></a>Train with target</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># labels for adversarial training</span><br><span class="line">target_label = 1</span><br><span class="line"></span><br><span class="line">pred_target1 = pred_target1.detach()</span><br><span class="line">pred_target2 = pred_target2.detach()</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred_target1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred_target2))</span><br><span class="line"></span><br><span class="line">loss_D1 = bce_loss(D_out1,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out1.data.size()).fill_(target_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D2 = bce_loss(D_out2,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out2.data.size()).fill_(target_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D1 = loss_D1 / args.iter_size / 2</span><br><span class="line">loss_D2 = loss_D2 / args.iter_size / 2</span><br><span class="line"></span><br><span class="line">loss_D1.backward()</span><br><span class="line">loss_D2.backward()</span><br><span class="line"></span><br><span class="line">loss_D_value1 += loss_D1.data.cpu().numpy()</span><br><span class="line">loss_D_value2 += loss_D2.data.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，z = 1</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GAN/" rel="tag"># GAN</a>
          
            <a href="/tags/pytorch/" rel="tag"># pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/20/leetcode体系/" rel="next" title="leetcode体系">
                <i class="fa fa-chevron-left"></i> leetcode体系
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/25/那些高山们/" rel="prev" title="那些高山们">
                那些高山们 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chu Lin</p>
              <p class="site-description motion-element" itemprop="description">去人迹罕至的地方，留下自己的足迹。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">82</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">63</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-Structure-and-Training"><span class="nav-number">1.</span> <span class="nav-text">Network Structure and Training</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Discriminator"><span class="nav-number">1.1.</span> <span class="nav-text">Discriminator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Segmentation-Network"><span class="nav-number">1.2.</span> <span class="nav-text">Segmentation Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-level-Adaptation-Model"><span class="nav-number">1.3.</span> <span class="nav-text">Multi-level Adaptation Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train"><span class="nav-number">1.4.</span> <span class="nav-text">Train</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-Function"><span class="nav-number">1.5.</span> <span class="nav-text">Loss Function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-Code"><span class="nav-number">2.</span> <span class="nav-text">Network Code</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Discriminitor"><span class="nav-number">2.1.</span> <span class="nav-text">Discriminitor</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Segmentation-Network-1"><span class="nav-number">2.2.</span> <span class="nav-text">Segmentation Network</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Train-Code"><span class="nav-number">3.</span> <span class="nav-text">Train Code</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-G"><span class="nav-number">3.1.</span> <span class="nav-text">Train G</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-with-source"><span class="nav-number">3.1.1.</span> <span class="nav-text">Train with source</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-with-target"><span class="nav-number">3.1.2.</span> <span class="nav-text">Train with target</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-D"><span class="nav-number">3.2.</span> <span class="nav-text">Train D</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-with-source-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">Train with source</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-with-target-1"><span class="nav-number">3.2.2.</span> <span class="nav-text">Train with target</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chu Lin</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
